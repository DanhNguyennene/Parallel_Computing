{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29e9d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb898de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench(name, func, warmup=True):\n",
    "    \"\"\"Benchmark a function with optional warmup\"\"\"\n",
    "    if warmup:\n",
    "        func()  # Warmup run\n",
    "    start = time.time()\n",
    "    result = func()\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"{name:40s}: {elapsed:8.6f} sec\")\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe5e18cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Matrix Size: 100×100\n",
      "============================================================\n",
      "NumPy (@)                               : 0.000021 sec\n",
      "PyTorch CPU                             : 0.000020 sec\n",
      "JAX (XLA)                               : 0.000041 sec\n",
      "\n",
      "============================================================\n",
      "Matrix Size: 1000×1000\n",
      "============================================================\n",
      "NumPy (@)                               : 0.004713 sec\n",
      "PyTorch CPU                             : 0.011616 sec\n",
      "JAX (XLA)                               : 0.003049 sec\n",
      "\n",
      "============================================================\n",
      "Matrix Size: 10000×10000\n",
      "============================================================\n",
      "NumPy (@)                               : 2.147201 sec\n",
      "NumPy (@)                               : 2.147201 sec\n",
      "PyTorch CPU                             : 2.200503 sec\n",
      "PyTorch CPU                             : 2.200503 sec\n",
      "JAX (XLA)                               : 1.709231 sec\n",
      "\n",
      "============================================================\n",
      "SUMMARY TABLE\n",
      "============================================================\n",
      "       Size  NumPy (s)  PyTorch (s)  JAX (s)     Best Best Library\n",
      "    100×100   0.000021     0.000020 0.000041 0.000020      PyTorch\n",
      "  1000×1000   0.004713     0.011616 0.003049 0.003049          JAX\n",
      "10000×10000   2.147201     2.200503 1.709231 1.709231          JAX\n",
      "\n",
      "JAX (XLA)                               : 1.709231 sec\n",
      "\n",
      "============================================================\n",
      "SUMMARY TABLE\n",
      "============================================================\n",
      "       Size  NumPy (s)  PyTorch (s)  JAX (s)     Best Best Library\n",
      "    100×100   0.000021     0.000020 0.000041 0.000020      PyTorch\n",
      "  1000×1000   0.004713     0.011616 0.003049 0.003049          JAX\n",
      "10000×10000   2.147201     2.200503 1.709231 1.709231          JAX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Test all three required matrix sizes\n",
    "sizes = [100, 1000, 10000]\n",
    "results = []\n",
    "\n",
    "for size in sizes:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Matrix Size: {size}×{size}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate matrices\n",
    "    np.random.seed(42)\n",
    "    A_np = np.random.rand(size, size).astype(np.float32)\n",
    "    B_np = np.random.rand(size, size).astype(np.float32)\n",
    "    A_torch = torch.tensor(A_np)\n",
    "    B_torch = torch.tensor(B_np)\n",
    "    \n",
    "    # Benchmark each library\n",
    "    numpy_time = bench(\"NumPy (@)\", lambda: A_np @ B_np)\n",
    "    pytorch_time = bench(\"PyTorch CPU\", lambda: A_torch @ B_torch)\n",
    "    jax_time = bench(\"JAX (XLA)\", lambda: jnp.dot(A_np, B_np).block_until_ready())\n",
    "    \n",
    "    results.append({\n",
    "        'Size': f'{size}×{size}',\n",
    "        'NumPy (s)': numpy_time,\n",
    "        'PyTorch (s)': pytorch_time,\n",
    "        'JAX (s)': jax_time,\n",
    "        'Best': min(numpy_time, pytorch_time, jax_time)\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Create comparison table\n",
    "df = pd.DataFrame(results)\n",
    "df['Best Library'] = df[['NumPy (s)', 'PyTorch (s)', 'JAX (s)']].idxmin(axis=1).str.replace(' (s)', '')\n",
    "print(df.to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c710505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM INFORMATION\n",
      "============================================================\n",
      "\n",
      "Python: 3.11.13\n",
      "NumPy: 2.3.3\n",
      "PyTorch: 2.9.0+cu130\n",
      "JAX: N/A\n",
      "\n",
      "CPU Cores: 20\n",
      "PyTorch Threads: 14\n",
      "\n",
      "BLAS Backend:\n",
      "Build Dependencies:\n",
      "  blas:\n",
      "    detection method: pkgconfig\n",
      "    found: true\n",
      "    include directory: /opt/_internal/cpython-3.11.13/lib/python3.11/site-packages/scipy_openblas64/include\n",
      "    lib directory: /opt/_internal/cpython-3.11.13/lib/python3.11/site-packages/scipy_openblas64/lib\n",
      "    name: scipy-openblas\n",
      "    openblas configuration: OpenBLAS 0.3.30  USE64BITINT DYNAMIC_ARCH NO_AFFINITY\n",
      "      Haswell MAX_THREADS=64\n",
      "    pc file directory: /project/.openblas\n",
      "    version: 0.3.30\n",
      "  lapack:\n",
      "    detection method: pkgconfig\n",
      "    found: true\n",
      "    include directory: /opt/_internal/cpython-3.11.13/lib/python3.11/site-packages/scipy_openblas64/include\n",
      "    lib directory: /opt/_internal/cpython-3.11.13/lib/python3.11/site-packages/scipy_openblas64/lib\n",
      "    name: scipy-openblas\n",
      "    openblas configuration: OpenBLAS 0.3.30  USE64BITINT DYNAMIC_ARCH NO_AFFINITY\n",
      "      Haswell MAX_THREADS=64\n",
      "    pc file directory: /project/.openblas\n",
      "    version: 0.3.30\n",
      "Compilers:\n",
      "  c:\n",
      "    commands: cc\n",
      "    linker: ld.bfd\n",
      "    name: gcc\n",
      "    version: 14.2.1\n",
      "  c++:\n",
      "    commands: c++\n",
      "    linker: ld.bfd\n",
      "    name: gcc\n",
      "    version: 14.2.1\n",
      "  cython:\n",
      "    commands: cython\n",
      "    linker: cython\n",
      "    name: cython\n",
      "    version: 3.1.3\n",
      "Machine Information:\n",
      "  build:\n",
      "    cpu: x86_64\n",
      "    endian: little\n",
      "    family: x86_64\n",
      "    system: linux\n",
      "  host:\n",
      "    cpu: x86_64\n",
      "    endian: little\n",
      "    family: x86_64\n",
      "    system: linux\n",
      "Python Information:\n",
      "  path: /tmp/build-env-mrl14dj5/bin/python\n",
      "  version: '3.11'\n",
      "SIMD Extensions:\n",
      "  baseline:\n",
      "  - SSE\n",
      "  - SSE2\n",
      "  - SSE3\n",
      "  found:\n",
      "  - SSSE3\n",
      "  - SSE41\n",
      "  - POPCNT\n",
      "  - SSE42\n",
      "  - AVX\n",
      "  - F16C\n",
      "  - FMA3\n",
      "  - AVX2\n",
      "  not found:\n",
      "  - AVX512F\n",
      "  - AVX512CD\n",
      "  - AVX512_KNL\n",
      "  - AVX512_KNM\n",
      "  - AVX512_SKX\n",
      "  - AVX512_CLX\n",
      "  - AVX512_CNL\n",
      "  - AVX512_ICL\n",
      "  - AVX512_SPR\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPython: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"JAX: {jnp.__version__ if hasattr(jnp, '__version__') else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nCPU Cores: {os.cpu_count()}\")\n",
    "print(f\"PyTorch Threads: {torch.get_num_threads()}\")\n",
    "\n",
    "# Check BLAS backend\n",
    "print(\"\\nBLAS Backend:\")\n",
    "if hasattr(np, 'show_config'):\n",
    "    np.show_config()\n",
    "else:\n",
    "    print(\"  Unable to determine NumPy BLAS backend\")\n",
    "    \n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f9407",
   "metadata": {},
   "source": [
    "## Analysis: Why Libraries Are Faster\n",
    "\n",
    "Production libraries achieve superior performance through:\n",
    "1. **SIMD Vectorization**: AVX2/AVX-512 instructions process 8-16 floats per instruction\n",
    "2. **Optimized BLAS**: Hand-tuned assembly kernels (Intel MKL, OpenBLAS)\n",
    "3. **Cache Blocking**: Adaptive tile sizes based on CPU cache hierarchy\n",
    "4. **Prefetching**: Hardware prefetch hints to reduce memory latency\n",
    "5. **Multi-threading**: Automatic parallelization across all cores\n",
    "\n",
    "Our educational implementations demonstrate parallel computing concepts without these low-level optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6d99d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: Optimized Libraries vs Our C++/OpenMP Implementations\n",
      "================================================================================\n",
      "\n",
      "100×100:\n",
      "  Best Library (PyTorch):       0.000020s\n",
      "  Our Best (OpenMP Naive (16 threads)):   0.001800s\n",
      "  Performance Gap (Our/Library):           88.82×\n",
      "\n",
      "1000×1000:\n",
      "  Best Library (JAX):       0.003049s\n",
      "  Our Best (OpenMP Naive (16 threads)):   0.100900s\n",
      "  Performance Gap (Our/Library):           33.09×\n",
      "\n",
      "10000×10000:\n",
      "  Best Library (JAX):       1.709231s\n",
      "  Our Best (OpenMP Naive (16 threads)):  57.550000s\n",
      "  Performance Gap (Our/Library):           33.67×\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Our implementation results (from actual test runs on same hardware - Machine 3)\n",
    "our_results = {\n",
    "    '100×100': {\n",
    "        'OpenMP Naive (1 thread)': 0.0070,\n",
    "        'OpenMP Naive (16 threads)': 0.0018,\n",
    "        'OpenMP Strassen (7 threads)': 0.0031\n",
    "    },\n",
    "    '1000×1000': {\n",
    "        'OpenMP Naive (1 thread)': 0.1897,\n",
    "        'OpenMP Naive (16 threads)': 0.1009,\n",
    "        'OpenMP Strassen (7 threads)': 0.1800\n",
    "    },\n",
    "    '10000×10000': {\n",
    "        'OpenMP Naive (1 thread)': 158.79,\n",
    "        'OpenMP Naive (16 threads)': 57.55,\n",
    "        'OpenMP Strassen (7 threads)': 118.78\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: Optimized Libraries vs Our C++/OpenMP Implementations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, size in enumerate(sizes):\n",
    "    size_str = f'{size}×{size}'\n",
    "    \n",
    "    # Get library results\n",
    "    numpy_time = results[i]['NumPy (s)']\n",
    "    pytorch_time = results[i]['PyTorch (s)']\n",
    "    jax_time = results[i]['JAX (s)']\n",
    "    lib_best = results[i]['Best']\n",
    "    \n",
    "    # Determine which library is best\n",
    "    if lib_best == numpy_time:\n",
    "        lib_best_name = 'NumPy'\n",
    "    elif lib_best == pytorch_time:\n",
    "        lib_best_name = 'PyTorch'\n",
    "    else:\n",
    "        lib_best_name = 'JAX'\n",
    "    \n",
    "    # Get our best result\n",
    "    our_best = min(our_results[size_str].values())\n",
    "    our_best_name = min(our_results[size_str].items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    print(f\"\\n{size_str}:\")\n",
    "    print(f\"  Best Library ({lib_best_name}):     {lib_best:10.6f}s\")\n",
    "    print(f\"  Our Best ({our_best_name}): {our_best:10.6f}s\")\n",
    "    print(f\"  Performance Gap (Our/Library):      {our_best/lib_best:10.2f}×\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84daa21",
   "metadata": {},
   "source": [
    "## Comprehensive Benchmark: All Matrix Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b90078",
   "metadata": {},
   "source": [
    "# Library Comparison Benchmarks\n",
    "\n",
    "This notebook compares our implementations against production-grade matrix multiplication libraries:\n",
    "- **NumPy**: Standard Python numerical library (uses BLAS/LAPACK)\n",
    "- **PyTorch**: Deep learning framework (uses MKL or OpenBLAS)\n",
    "- **JAX**: Google's high-performance numerical computing library (XLA compiler)\n",
    "\n",
    "We test three matrix sizes: 100×100, 1000×1000, and 10000×10000 to match our C++/OpenMP/MPI implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c515f0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "764bdd79",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "704cba8b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
