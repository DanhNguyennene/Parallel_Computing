\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{float}
\usepackage{multirow}

\geometry{margin=1in}

% Code listing settings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\geometry{margin=2cm}
\pagestyle{fancy}
\fancyhf{}
\rfoot{Page \thepage}
\lhead{Multidiscipline Project CSE}
% Header and footer
\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\renewcommand{\headrulewidth}{0pt}

\lstset{
    language=C++,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{purple}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{black},
    stepnumber=1,
    numbersep=8pt,
    backgroundcolor=\color{gray!5},
    frame=single,
    frameround=tttt,
    tabsize=4,
    breaklines=true,
    breakatwhitespace=true,
    showstringspaces=false,
    xleftmargin=15pt,
    xrightmargin=5pt
}

% Title formatting
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

\begin{document}

% Title Page
\begin{titlepage}
    \centering
    
    \vspace*{2cm}
    
    {\Large\textbf{HO CHI MINH CITY, UNIVERSITY OF TECHNOLOGY}}\\
    \vspace{0.5cm}
    {\large\textbf{DEPARTMENT OF COMPUTER SCIENCE AND ENGINEER}}\\
    
    \vspace{1cm}
    

   \begin{center}
    \vspace{1cm}  % Top padding
    \includegraphics[width=0.3\textwidth]{logo.png}
    \vspace{1cm}  % Bottom padding
\end{center}
    {\Large\textbf{Assignment Report Group 4 CO3067}}\\
    \vspace{1cm}
    {\Huge\textbf{Parallel Computing}}\\
    \vspace{0.5cm}
    {\Large\textbf{Semester: 251}}\\
    
    \vspace{3cm}
    
    \begin{tabular}{ll}
        \textbf{Students:} & Théo Bloch - 2460078 \\
                          & Nguyen Phuc Thanh Danh - 2252102\\
    \end{tabular}
    
    \vfill
    
    {\large\textbf{HO CHI MINH CITY}}\\
    
\end{titlepage}


\newpage
\tableofcontents
\newpage

\section{Introduction}

This document provides comprehensive documentation for parallel matrix multiplication implementations using different parallel programming paradigms: MPI (Message Passing Interface), OpenMP (Open Multi-Processing), and hybrid MPI+OpenMP approaches. The project implements both naive (standard) and Strassen's algorithm for matrix multiplication.

\subsection{Project Objectives}

\begin{itemize}
    \item Implement parallel matrix multiplication using MPI for distributed memory systems
    \item Implement parallel matrix multiplication using OpenMP for shared memory systems
    \item Implement hybrid MPI+OpenMP approach combining both paradigms
    \item Compare naive and Strassen's algorithm performance
    \item Benchmark and analyze scalability across different problem sizes
\end{itemize}

\subsection{Algorithms Implemented}

\begin{enumerate}
    \item \textbf{Naive Matrix Multiplication}: Standard $O(n^3)$ algorithm
    \item \textbf{Strassen's Algorithm}: Divide-and-conquer approach with $O(n^{2.807})$ complexity
\end{enumerate}

\section{MPI Implementation}

The Message Passing Interface (MPI) implementations leverage distributed memory parallelism, allowing matrix multiplication to scale across multiple nodes in a cluster. We implemented two variants: a straightforward naive algorithm using standard triple-nested loops, and Strassen's divide-and-conquer algorithm adapted for distributed execution.

\subsection{MPI Naive Matrix Multiplication}

\textbf{Mathematical Formulation:}

For matrices $A, B \in \mathbb{R}^{N \times N}$, the matrix product $C = AB$ is defined as:
\[
C_{ij} = \sum_{k=1}^{N} A_{ik} \cdot B_{kj}, \quad \forall i,j \in \{1, \ldots, N\}
\]

With $p$ MPI processes, we distribute the computation:
\begin{itemize}
    \item Process $r$ computes rows $[r \cdot \frac{N}{p}, (r+1) \cdot \frac{N}{p})$ of result matrix $C$
    \item Sequential complexity: $T_{\text{seq}} = \mathcal{O}(N^3)$
    \item Parallel computation per process: $T_{\text{comp}} = \mathcal{O}(\frac{N^3}{p})$
    \item Communication time: $T_{\text{comm}} = \mathcal{O}(N^2)$ for broadcast + $\mathcal{O}(\frac{N^2}{p})$ for scatter/gather
\end{itemize}

\textbf{Theoretical Speedup:}
\[
S(p) = \frac{T_{\text{seq}}}{T_{\text{comp}} + T_{\text{comm}}} = \frac{\mathcal{O}(N^3)}{\mathcal{O}(\frac{N^3}{p}) + \mathcal{O}(N^2)} \approx p \quad \text{for } N \gg \sqrt{p}
\]

\textbf{Parallel Efficiency:}
\[
\eta(p) = \frac{S(p)}{p} = \frac{1}{1 + \frac{p \cdot T_{\text{comm}}}{T_{\text{comp}}}} \to 1 \quad \text{as } N \to \infty
\]

The MPI naive implementation employs a \textbf{pipelined ring communication pattern} combined with Z-order curve blocking for cache optimization. Unlike traditional scatter-broadcast-gather approaches, this implementation uses point-to-point communication to distribute matrix rows from rank 0 to all processes sequentially, followed by a collective broadcast of matrix $B$.

\textbf{Algorithm Overview:}

The \texttt{pipelinedRingMultiply} function implements the following strategy:
\begin{enumerate}
    \item \textbf{Row Distribution}: Rank 0 sends rows of matrix $A$ to processes sequentially using \texttt{MPI\_Send}, with each process receiving exactly $N/p$ rows
    \item \textbf{Matrix B Broadcast}: The entire matrix $B$ is broadcast to all processes using \texttt{MPI\_Bcast}
    \item \textbf{Local Computation}: Each process performs matrix multiplication on its local rows using Z-order blocking
    \item \textbf{Result Collection}: Processes send their computed rows back to rank 0 using \texttt{MPI\_Send}
\end{enumerate}

\textbf{Key Implementation Features:}
\begin{itemize}
    \item \textbf{Z-Order Curve Blocking}: Uses Morton curve (bit interleaving) to improve cache locality during computation
    \item \textbf{Point-to-Point Communication}: Sequential distribution pattern with explicit \texttt{MPI\_Send/Recv}
    \item \textbf{Divisibility Constraint}: Matrix size $N$ must be divisible by number of processes $p$
    \item \textbf{Load Balancing}: Equal row distribution ensures uniform workload ($N/p$ rows per process)
    \item \textbf{Verification}: Optional serial verification on rank 0 for correctness checking
\end{itemize}

\textbf{Z-Order Blocking for Cache Optimization:}

The \texttt{zOrderMultiply} function implements space-filling curve traversal using \textbf{Morton encoding} (also called Z-order curve). This technique interleaves the bits of two 2D coordinates (x, y) to create a single integer that preserves spatial locality, meaning nearby points in 2D space remain close in the 1D memory representation.

\textbf{Why Z-Order Curve?}

Traditional row-major or column-major matrix layouts can cause poor cache performance during matrix multiplication because:
\begin{itemize}
    \item Sequential access to one matrix (e.g., row of A) may conflict with strided access to another (e.g., column of B)
    \item Cache lines are wasted when only accessing single elements from different rows
    \item For large matrices, distant rows/columns may evict each other from cache
\end{itemize}

Z-order curve solves this by organizing matrix elements in a fractal pattern that keeps spatially nearby elements close in memory, improving cache hit rates by 2-3× for large matrices.

\textbf{Bit Interleaving Algorithm:}

\begin{lstlisting}[language=C++]
inline unsigned int interleaveBits(unsigned int x, unsigned int y) {
    // Step 1: Spread x bits to create gaps for y bits
    x = (x | (x << 8)) & 0x00FF00FF;  // 8-bit gaps
    x = (x | (x << 4)) & 0x0F0F0F0F;  // 4-bit gaps
    x = (x | (x << 2)) & 0x33333333;  // 2-bit gaps
    x = (x | (x << 1)) & 0x55555555;  // 1-bit gaps
    
    // Step 2: Same expansion for y
    y = (y | (y << 8)) & 0x00FF00FF;
    y = (y | (y << 4)) & 0x0F0F0F0F;
    y = (y | (y << 2)) & 0x33333333;
    y = (y | (y << 1)) & 0x55555555;
    
    // Step 3: Interleave - y in odd positions, x in even
    return x | (y << 1);
}
\end{lstlisting}

\textbf{How Bit Interleaving Works - Example with x=5, y=3:}

\begin{enumerate}
    \item \textbf{Initial state:}
    \begin{itemize}
        \item x = 5 = \texttt{00000101} (binary)
        \item y = 3 = \texttt{00000011} (binary)
    \end{itemize}
    
    \item \textbf{Step 1 - Create 8-bit gaps:} \texttt{x = (x | (x << 8)) \& 0x00FF00FF}
    \begin{itemize}
        \item Shift left 8 bits and OR with original spreads lower 8 bits apart
        \item Mask \texttt{0x00FF00FF} = \texttt{00000000111111110000000011111111} keeps alternating 8-bit blocks
        \item Result: bits now have 8-bit spacing
    \end{itemize}
    
    \item \textbf{Step 2 - Create 4-bit gaps:} \texttt{x = (x | (x << 4)) \& 0x0F0F0F0F}
    \begin{itemize}
        \item Shift left 4 bits and OR spreads to 4-bit spacing
        \item Mask \texttt{0x0F0F0F0F} = \texttt{00001111000011110000111100001111} keeps alternating 4-bit blocks
    \end{itemize}
    
    \item \textbf{Step 3 - Create 2-bit gaps:} \texttt{x = (x | (x << 2)) \& 0x33333333}
    \begin{itemize}
        \item Mask \texttt{0x33333333} = \texttt{00110011001100110011001100110011} keeps alternating 2-bit blocks
    \end{itemize}
    
    \item \textbf{Step 4 - Create 1-bit gaps:} \texttt{x = (x | (x << 1)) \& 0x55555555}
    \begin{itemize}
        \item Mask \texttt{0x55555555} = \texttt{01010101010101010101010101010101} keeps only even bit positions
        \item Final x: each original bit now has a gap for y bits: \texttt{0\_1\_0\_1\_} (underscores = gaps)
    \end{itemize}
    
    \item \textbf{Step 5 - Combine:} Apply same expansion to y, then \texttt{return x | (y << 1)}
    \begin{itemize}
        \item x expanded: \texttt{0\_1\_0\_1\_} (even positions: 0, 2, 4, 6...)
        \item y expanded and shifted: \texttt{\_0\_0\_1\_1} (odd positions: 1, 3, 5, 7...)
        \item Interleaved result: \texttt{00100111} = 39 (decimal)
    \end{itemize}
\end{enumerate}

\textbf{Visual Pattern - Z-Order Traversal:}

For a 4×4 matrix, Z-order indices create this access pattern:
\begin{verbatim}
Traditional row-major:     Z-order curve:
0  1  2  3                 0  1  4  5
4  5  6  7                 2  3  6  7
8  9  10 11                8  9  12 13
12 13 14 15                10 11 14 15
\end{verbatim}

Notice how the Z-order pattern keeps nearby elements (e.g., 0,1,2,3) spatially close, forming a recursive "Z" shape at each scale. This means a 64-byte cache line can hold elements that are actually neighbors in 2D space.

\textbf{Performance Benefits:}

\begin{itemize}
    \item \textbf{Cache Locality}: Elements accessed together in matrix multiplication (like A[i][k] and B[k][j]) are more likely to be in the same cache line
    \item \textbf{TLB Efficiency}: Reduces Translation Lookaside Buffer misses by accessing memory pages more uniformly
    \item \textbf{Prefetcher Friendly}: CPU hardware prefetchers can better predict access patterns
    \item \textbf{Typical Speedup}: 1.5-3× improvement for matrices larger than L3 cache size (>8MB)
\end{itemize}

This Z-order curve implementation is particularly effective for distributed computing where each MPI process works on a local block, as it minimizes the working set size and maximizes cache utilization during the triple-nested loop computation.

\textbf{File: mpi-naive/mpi-naive.h}
\begin{lstlisting}[language=C++]
#ifndef MPI_NAIVE_H
#define MPI_NAIVE_H

#include <mpi.h>
#include <iostream>
#include <ctime>
#include <vector>
#include <cstdlib>
#include <cmath>
#include <iomanip>

void initializeMatrices(int N, int rank, 
                       std::vector<int>& A, 
                       std::vector<int>& B, 
                       std::vector<int>& C);

void pipelinedRingMultiply(int N, int rank, int size,
                          const std::vector<int>& A,
                          const std::vector<int>& B,
                          std::vector<int>& C,
                          double& comp_time);

void zOrderMultiply(int N, int rank, int size,
                   const std::vector<int>& A_local,
                   int local_rows,
                   const std::vector<int>& B,
                   std::vector<int>& C_local,
                   int block_size);

void gatherResults(int N, int rank, int rows_per_proc, 
                  const std::vector<int>& local_c, 
                  std::vector<int>& C);

double computeMaxLocalTime(double local_time, int rank);

void serialVerify(int N, const std::vector<int>& A, 
                 const std::vector<int>& B, 
                 std::vector<int>& C_verify);

bool verifyResults(int N, const std::vector<int>& C, 
                  const std::vector<int>& C_verify, 
                  int rank);

#endif
\end{lstlisting}

\textbf{Key Functions:}

\begin{itemize}
    \item \texttt{pipelinedRingMultiply()}: Main coordination function implementing the ring pattern
    \item \texttt{zOrderMultiply()}: Cache-optimized local computation using Morton curve blocking
    \item \texttt{interleaveBits()}: Converts 2D coordinates to 1D Z-order index via bit interleaving
    \item \texttt{deinterleaveBits()}: Inverse operation for coordinate recovery
\end{itemize}

\textbf{Matrix Distribution Strategy:}

\begin{equation}
\text{rows\_per\_process} = \frac{N}{p}, \quad N \bmod p = 0
\end{equation}

Process $i$ (where $i = 0, 1, \ldots, p-1$) receives rows $[i \times \frac{N}{p}, (i+1) \times \frac{N}{p})$ of matrix $A$.

\textbf{Communication Pattern:}

\begin{enumerate}
    \item \textbf{Sequential Send}: Rank 0 sends matrix $A$ rows to processes 1 through $p-1$ using \texttt{MPI\_Send}
    \item \textbf{Collective Broadcast}: Entire matrix $B$ ($N^2$ elements) broadcast via \texttt{MPI\_Bcast}
    \item \textbf{Local Computation}: Each process computes $C_{local} = A_{local} \times B$ with Z-order blocking
    \item \textbf{Sequential Receive}: Rank 0 receives results from processes 1 through $p-1$ using \texttt{MPI\_Recv}
\end{enumerate}

The computational complexity is $O(n^3/p)$ per process, while communication overhead is $O(n^2)$ dominated by the broadcast of matrix $B$.

\subsection{MPI Strassen Matrix Multiplication}

\textbf{Algorithm Overview:}

Strassen's algorithm reduces matrix multiplication from 8 to 7 recursive multiplications, achieving subquadratic asymptotic complexity.

\textbf{Complexity Analysis:}

The recurrence relation:
\[
T(N) = 7T\left(\frac{N}{2}\right) + \Theta(N^2)
\]

By Master Theorem (Case 1): $a=7, b=2, \log_b a = \log_2 7 \approx 2.807 > 2$
\[
T(N) = \Theta(N^{\log_2 7}) \approx \Theta(N^{2.807})
\]

\textbf{Parallel Model with 7 MPI Processes:}
\begin{itemize}
    \item Sequential: 7 products computed serially $\to 7 \cdot T_{\text{mult}}$
    \item Parallel: 7 products computed simultaneously $\to T_{\text{mult}}$ (ideal)
    \item Speedup per recursion level: $S \approx 7$
    \item Communication overhead: $\mathcal{O}(N^2)$ for data distribution + result assembly
    \item Total parallel time: $T_{\text{par}} = T_{\text{mult}} + \mathcal{O}(N^2)$
\end{itemize}

\textbf{Comparison with Naive:}
\begin{itemize}
    \item Naive: $\mathcal{O}(N^3)$ operations
    \item Strassen: $\mathcal{O}(N^{2.807})$ operations
    \item Crossover point: typically $N \approx 512$ to $N \approx 2048$
    \item Below threshold (128): switches to naive for better cache performance
\end{itemize}

\textbf{The Seven Products:}

For matrices partitioned into blocks:
\[
A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}, \quad
B = \begin{bmatrix} B_{11} & B_{12} \\ B_{21} & B_{22} \end{bmatrix}
\]

The seven products are:
\begin{align}
M_1 &= (A_{11} + A_{22})(B_{11} + B_{22}) \\
M_2 &= (A_{21} + A_{22})B_{11} \\
M_3 &= A_{11}(B_{12} - B_{22}) \\
M_4 &= A_{22}(B_{21} - B_{11}) \\
M_5 &= (A_{11} + A_{12})B_{22} \\
M_6 &= (A_{21} - A_{11})(B_{11} + B_{12}) \\
M_7 &= (A_{12} - A_{22})(B_{21} + B_{22})
\end{align}

Result matrix blocks:
\begin{align}
C_{11} &= M_1 + M_4 - M_5 + M_7 \\
C_{12} &= M_3 + M_5 \\
C_{21} &= M_2 + M_4 \\
C_{22} &= M_1 - M_2 + M_3 + M_6
\end{align}

\textbf{MPI Parallelization Strategy:}

The implementation uses exactly 7 processes, one for each Strassen product:

\begin{itemize}
    \item \textbf{Process 0}: Coordinates and computes $M_7$
    \item \textbf{Process 1}: Computes $M_1$
    \item \textbf{Process 2}: Computes $M_2$
    \item \textbf{Process 3}: Computes $M_3$
    \item \textbf{Process 4}: Computes $M_4$
    \item \textbf{Process 5}: Computes $M_5$
    \item \textbf{Process 6}: Computes $M_6$
\end{itemize}

\textbf{Implementation Structure:}

\textbf{File: mpi-strassen/mpi-strassen.h}
\begin{lstlisting}[language=C++]
#ifndef MPI_STRASSEN_H
#define MPI_STRASSEN_H

#include <mpi.h>
#include <iostream>
#include <cmath>
#include <chrono>
#include <vector>
#include <random>
#include <cstring>

#define LOWER_B 0.0
#define UPPER_B 1.0
#define THRESHOLD 128

class Timer {
    std::chrono::high_resolution_clock::time_point start_;
public:
    void start() { 
        start_ = std::chrono::high_resolution_clock::now(); 
    }
    float elapse() {
        auto end = std::chrono::high_resolution_clock::now();
        return std::chrono::duration<float>(end - start_).count();
    }
};

std::vector<float> createRandomMatrix(int size, int seed);

void naiveMultiply(int n, const float *A, int lda,
                   const float *B, int ldb,
                   float *C, int ldc);

void addMatrix(int n, const float *A, int lda,
               const float *B, int ldb,
               float *C, int ldc);

void subtractMatrix(int n, const float *A, int lda,
                    const float *B, int ldb,
                    float *C, int ldc);

void strassenSerial(int n, const float *A, int lda,
                    const float *B, int ldb,
                    float *C, int ldc,
                    float *work);

void strassen_mpi_wrapper(int N, int rank, int numProcs,
                         int *sendcounts, int *displs,
                         const float *A, int lda,
                         const float *B, int ldb,
                         float *C, int ldc);

#endif
\end{lstlisting}

\section{OpenMP Implementation}

The OpenMP implementations leverage shared-memory parallelism using thread-based task decomposition. OpenMP provides a simpler programming model compared to MPI, as all threads share the same address space. We implemented both naive and Strassen algorithms using OpenMP's task-based parallelism combined with recursive divide-and-conquer strategies for optimal load balancing.

\subsection{OpenMP Naive Implementation}

\textbf{Mathematical Model for Tiling:}

For tile size $B$ (typically 128) and cache size $M$:
\begin{itemize}
    \item Matrix partitioned into $\lceil \frac{N}{B} \rceil^2$ tiles
    \item Working set per tile: $3B^2$ elements (from $A$, $B$, $C$)
    \item Optimal tile size: $B = \lfloor \sqrt{M/3} \rfloor$ to fit in L2 cache
    \item Cache misses: $\mathcal{O}(\frac{N^3}{B \cdot L})$ where $L$ is cache line size
    \item Improvement: $\sim B\times$ reduction in cache misses vs. naive
\end{itemize}

\textbf{Parallel Complexity with $t$ Threads:}
\[
T_{\text{par}}(t) = \frac{\mathcal{O}(N^3)}{t} + \mathcal{O}(\frac{N^2}{B^2} \cdot t_{\text{sched}}) + \mathcal{O}(t \log t)
\]

where:
\begin{itemize}
    \item First term: ideal parallel computation
    \item Second term: dynamic scheduling overhead ($t_{\text{sched}} \approx 1\mu s$ per task)
    \item Third term: thread synchronization cost
\end{itemize}

\textbf{Theoretical Speedup:}
\[
S(t) = \frac{T_{\text{seq}}}{T_{\text{par}}(t)} \approx \frac{t}{1 + \frac{c \cdot t}{N}} \quad \text{where } c = \frac{N^2 \cdot t_{\text{sched}}}{B^2 \cdot t_{\text{comp}}}
\]

The OpenMP naive implementation uses a cache-optimized tiled matrix multiplication approach with OpenMP parallel for loops. Rather than using divide-and-conquer recursion, this implementation employs blocking (tiling) to improve cache locality and combines it with dynamic scheduling for better load balancing across threads. The implementation uses the standard $O(n^3)$ algorithm but optimizes memory access patterns through tiling and loop reordering.

\textbf{Algorithm Description:}

The OpenMP naive implementation uses a tiled matrix multiplication strategy with the i-k-j loop ordering. This loop order is particularly effective for cache performance as it allows vectorization of the innermost loop and maintains good temporal locality for the result matrix.

\textbf{Tiled Matrix Multiplication:}

For matrices $A$, $B$, and $C$ of size $n \times n$, the computation is divided into tiles of size $b \times b$:

\[
C[i][j] = \sum_{k=0}^{n-1} A[i][k] \cdot B[k][j]
\]

The tiled approach processes the matrices in blocks, improving cache utilization:

\begin{lstlisting}[language=C++]
for (ii = 0; ii < n; ii += tile_size)          // Tile row
  for (jj = 0; jj < n; jj += tile_size)        // Tile column
    for (kk = 0; kk < n; kk += tile_size)      // Tile inner dimension
      for (i = ii; i < min(ii+tile_size, n); i++)
        for (k = kk; k < min(kk+tile_size, n); k++)
          for (j = jj; j < min(jj+tile_size, n); j++)
            C[i][j] += A[i][k] * B[k][j]
\end{lstlisting}

\textbf{OpenMP Parallelization Strategy:}

\begin{lstlisting}[language=C++]
void tiledMatMul(int n, const float *A, const float *B, 
                 float *C, int num_threads, int tile_size) {
    omp_set_num_threads(num_threads);
    std::fill(C, C + n * n, 0.0f);

#pragma omp parallel for collapse(2) schedule(dynamic)
    for (int ii = 0; ii < n; ii += tile_size) {
        for (int jj = 0; jj < n; jj += tile_size) {
            for (int kk = 0; kk < n; kk += tile_size) {
                int i_end = std::min(ii + tile_size, n);
                int j_end = std::min(jj + tile_size, n);
                int k_end = std::min(kk + tile_size, n);

                for (int i = ii; i < i_end; ++i) {
                    for (int k = kk; k < k_end; ++k) {
                        float a_ik = A[i * n + k];
#pragma omp simd
                        for (int j = jj; j < j_end; ++j) {
                            C[i * n + j] += a_ik * B[k * n + j];
                        }
                    }
                }
            }
        }
    }
}
\end{lstlisting}

\textbf{Key Optimizations:}

\begin{itemize}
    \item \textbf{collapse(2):} Parallelizes both outer tile loops for better work distribution
    \item \textbf{schedule(dynamic):} Dynamically assigns tiles to threads for load balancing
    \item \textbf{i-k-j ordering:} Optimizes cache access patterns (temporal locality for C, spatial locality for B)
    \item \textbf{SIMD vectorization:} \texttt{\#pragma omp simd} enables automatic vectorization of the innermost loop
    \item \textbf{Register blocking:} The \texttt{a\_ik} scalar is reused across the innermost loop
    \item \textbf{Adaptive tile size:} Default 128×128 tiles balance cache usage and parallelism overhead
\end{itemize}

\textbf{Cache Locality Analysis:}

The i-k-j loop ordering provides superior cache performance:
\begin{itemize}
    \item Matrix $C[i][j]$: Written sequentially (write-back cache friendly)
    \item Matrix $A[i][k]$: Each element reused $n$ times (stored in register)
    \item Matrix $B[k][j]$: Read sequentially enabling cache line prefetching
\end{itemize}

For typical L1 cache sizes (32-64KB), tiles of 128×128 floats (64KB) fit comfortably, minimizing cache misses.

\subsection{OpenMP Strassen Implementation}

The OpenMP Strassen implementation adapts the seven-product algorithm for shared-memory parallelism. Each recursive call potentially spawns OpenMP tasks for the seven products, allowing the runtime to dynamically schedule work across available threads. This implementation includes sophisticated optimizations such as adaptive thresholding and cache-aware base cases to maximize performance across different matrix sizes and core counts.

\textbf{Parallel Strategy:}

The OpenMP Strassen implementation uses:
\begin{itemize}
    \item Task-based parallelism for the 7 Strassen products
    \item Depth-limited recursion to control task creation overhead
    \item SIMD vectorization for base case computations
\end{itemize}

\textbf{Implementation Highlights:}

\textbf{File: openmp-strassen/openmap-strassen.h}
\begin{lstlisting}[language=C++]
void strassenParallel(int n, const float *A, int lda,
                      const float *B, int ldb,
                      float *C, int ldc,
                      int depth, int max_depth, int threshold) {
    if (depth >= max_depth || n % 2 != 0) {
        if (n % 2 == 0) {
            size_t stackSize = (size_t)(3 * n * n);
            std::vector<float> serialStack(stackSize);
            strassenSerial(n, A, lda, B, ldb, C, ldc, 
                          serialStack.data(), threshold);
        } else {
            naiveMultiply(n, A, lda, B, ldb, C, ldc);
        }
        return;
    }
    
    int m = n / 2;
    std::vector<float> results(7 * m * m);
    
    #pragma omp taskgroup
    {
        // Create 7 tasks for M1-M7
        #pragma omp task shared(results)
        {
            // Compute M2 = (A21 + A22)B11
            std::vector<float> T(m * m);
            addMatrix(m, A21, lda, A22, lda, T.data(), m);
            strassenParallel(m, T.data(), m, B11, ldb, 
                           M2, m, depth + 1, max_depth, threshold);
        }
        // ... remaining 6 tasks
    }
    
    // Combine results
    #pragma omp parallel for collapse(2)
    for (int i = 0; i < m; i++) {
        for (int j = 0; j < m; j++) {
            int k = i * m + j;
            C11[k] = M1[k] + M4[k] - M5[k] + M7[k];
            C12[k] = M3[k] + M5[k];
            C21[k] = M2[k] + M4[k];
            C22[k] = M1[k] - M2[k] + M3[k] + M6[k];
        }
    }
}
\end{lstlisting}

\section{Hybrid MPI+OpenMP Implementation}

\subsection{Architecture Overview}

\textbf{Two-Level Parallel Model:}

For $p$ MPI processes and $t$ OpenMP threads per process:

\textbf{Total Parallelism:}
\[
\Pi_{\text{total}} = p \times t
\]

\textbf{Hierarchical Computation:}
\begin{itemize}
    \item \textbf{Level 1 (MPI)}: Coarse-grained parallelism across $p=7$ processes
    \item \textbf{Level 2 (OpenMP)}: Fine-grained parallelism with $t$ threads per process
    \item Per-process work: $W_{\text{proc}} = \frac{W_{\text{total}}}{p}$
    \item Per-thread work: $W_{\text{thread}} = \frac{W_{\text{total}}}{p \cdot t}$ (ideal)
\end{itemize}

\textbf{Communication Model:}
\[
T_{\text{comm}} = T_{\text{MPI}} + T_{\text{OpenMP}}
\]

where:
\begin{itemize}
    \item $T_{\text{MPI}} = \alpha_{\text{MPI}} + \beta_{\text{MPI}} \cdot \frac{N^2}{p}$: Inter-node communication (network latency $\alpha$, bandwidth $\beta$)
    \item $T_{\text{OpenMP}} = \mathcal{O}(t \log t)$: Intra-node synchronization (shared memory)
    \item Typically: $T_{\text{MPI}} \gg T_{\text{OpenMP}}$ due to network bottleneck
\end{itemize}

\textbf{Parallel Efficiency:}
\[
\eta_{\text{hybrid}} = \frac{T_{\text{seq}}}{p \cdot t \cdot T_{\text{par}}} = \frac{1}{1 + \frac{T_{\text{MPI}}}{T_{\text{comp}}/p} + \frac{T_{\text{OpenMP}}}{T_{\text{comp}}/(p \cdot t)}}
\]

The hybrid implementation represents the most sophisticated parallelization strategy, combining distributed-memory MPI for inter-node communication with shared-memory OpenMP for intra-node parallelism. This two-level approach is particularly effective on modern HPC clusters where each node contains multiple cores. The implementation uses 7 MPI processes (matching Strassen's requirement), with each process spawning multiple OpenMP threads to fully utilize available hardware resources.

\textbf{Key architectural components:}
\begin{itemize}
    \item \textbf{MPI Layer}: Distributes the seven Strassen products across processes, handles inter-node data movement
    \item \textbf{OpenMP Layer}: Each MPI process uses OpenMP threads to parallelize its assigned matrix product computation
    \item \textbf{Load Balancing}: MPI provides coarse-grained parallelism (7-way), OpenMP provides fine-grained parallelism within each process
    \item \textbf{Memory Efficiency}: Shared memory within nodes reduces communication overhead compared to pure MPI
\end{itemize}

\textbf{Communication Strategy:}

The communication pattern carefully minimizes data movement while ensuring all processes have the necessary submatrices:

\begin{enumerate}
    \item \textbf{Matrix Distribution}: Rank 0 packs submatrices and scatters to 7 processes
    \item \textbf{Local Computation}: Each process uses OpenMP to compute its Strassen product
    \item \textbf{Result Collection}: MPI\_Gather collects results to rank 0
    \item \textbf{Final Combination}: Rank 0 combines the 7 products into final result
\end{enumerate}

\subsection{Thread Safety}

The implementation ensures thread safety through:
\begin{itemize}
    \item Thread-local storage for temporary matrices
    \item Task-based parallelism avoiding race conditions
    \item Proper synchronization using \texttt{\#pragma omp taskwait}
\end{itemize}


\section{GPU Shader Implementations}

The GPU implementations leverage OpenGL compute shaders to perform matrix multiplication on the GPU. All implementations use headless EGL context initialization for deployment on systems without display servers (e.g., compute clusters).

\subsection{Naive Shader}
The \textit{naive} shader implements the straightforward element-wise matrix multiplication using the standard triple-nested loop approach:
\[
C[i,j] = \sum_{k=0}^{N-1} A[i,k] \cdot B[k,j]
\]

\textbf{Implementation Details:}
\begin{itemize}
    \item Each thread computes a single element of the output matrix \(C\)
    \item Direct global memory access pattern: \texttt{A[row * N + k]} and \texttt{B[k * N + col]}
    \item No shared memory usage
    \item Thread indices map directly to output coordinates: \texttt{(gl\_GlobalInvocationID.x, gl\_GlobalInvocationID.y)}
    \item Work group size: 16×16 threads
\end{itemize}

\textbf{Performance Characteristics:}
\begin{itemize}
    \item \textbf{Pros:}
    \begin{itemize}
        \item Simple and easy to implement
        \item Can handle arbitrary matrix sizes without padding
        \item Minimal synchronization overhead
    \end{itemize}
    \item \textbf{Cons:}
    \begin{itemize}
        \item High global memory bandwidth requirement ($O(n^3)$ accesses)
        \item Poor memory coalescing for matrix $A$ accesses
        \item GPU driver timeouts on very large matrices (e.g., $16384\times16384$) due to long kernel execution
        \item No cache reuse between adjacent threads
    \end{itemize}
\end{itemize}

\subsection{Chunked (Tiled) Shader}
The \textit{chunked} shader employs tiling and shared memory optimization. The matrix computation is divided into $TILE \times TILE$ blocks (where $TILE=16$), and each workgroup cooperatively loads tiles into fast shared memory before computation.

\textbf{Algorithm:}
\[
C[i,j] = \sum_{t=0}^{\lceil N/TILE \rceil - 1} \sum_{k=0}^{TILE-1} A_{tile}[i,k] \cdot B_{tile}[k,j]
\]

\textbf{Implementation Details:}
\begin{itemize}
    \item \textbf{Shared Memory:} Each workgroup declares \texttt{shared float Asub[16][16]} and \texttt{Bsub[16][16]}
    \item \textbf{Cooperative Loading:} All threads in a workgroup collaboratively load one $16\times16$ tile from global to shared memory
    \item \textbf{Synchronization:} \texttt{memoryBarrierShared()} and \texttt{barrier()} ensure all threads see loaded data
    \item \textbf{Computation:} Inner loop accesses only shared memory: \texttt{sum += Asub[ly][k] * Bsub[k][lx]}
    \item \textbf{Iteration:} Outer loop iterates over $\lceil N/16 \rceil$ tiles
\end{itemize}

\textbf{Performance Characteristics:}
\begin{itemize}
    \item \textbf{Pros:}
    \begin{itemize}
        \item Reduces global memory traffic by factor of $TILE$ (16×)
        \item Enables memory coalescing for both $A$ and $B$
        \item Shared memory provides 100× bandwidth vs global memory
        \item 1.2× faster than naive on medium matrices (1024×1024)
        \item 1.36× faster than naive on large matrices (8192×8192)
    \end{itemize}
    \item \textbf{Cons:}
    \begin{itemize}
        \item Requires barrier synchronization (small overhead)
        \item Optimal only for power-of-2 matrix sizes (padding needed otherwise)
        \item Shared memory capacity limits tile size
    \end{itemize}
\end{itemize}

\subsection{Strassen GPU Shader}
The \textit{Strassen} shader implements a single-level Strassen algorithm optimized for GPU execution. Unlike recursive CPU implementations, this version uses a flat tiled approach to compute Strassen products efficiently.

\textbf{Algorithm Overview:}

The Strassen algorithm reduces matrix multiplication from 8 recursive products to 7, achieving $O(n^{2.807})$ complexity. For matrices partitioned as:
\[
A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}, \quad
B = \begin{bmatrix} B_{11} & B_{12} \\ B_{21} & B_{22} \end{bmatrix}
\]

The seven products are:
\begin{align*}
M_1 &= (A_{11} + A_{22})(B_{11} + B_{22}) \\
M_2 &= (A_{21} + A_{22})B_{11} \\
M_3 &= A_{11}(B_{12} - B_{22}) \\
M_4 &= A_{22}(B_{21} - B_{11}) \\
M_5 &= (A_{11} + A_{12})B_{22} \\
M_6 &= (A_{21} - A_{11})(B_{11} + B_{12}) \\
M_7 &= (A_{12} - A_{22})(B_{21} + B_{22})
\end{align*}

\textbf{GPU Implementation Strategy:}

Unlike the recursive CPU implementation, the GPU shader uses a \textit{single-level optimized tiled approach}:

\begin{itemize}
    \item \textbf{Tiling:} Similar to chunked shader, uses $16\times16$ shared memory tiles
    \item \textbf{Strassen Operations:} Supports offset and sign parameters for computing $M_1$ through $M_7$
    \item \textbf{Uniform Parameters:}
    \begin{itemize}
        \item \texttt{offsetA, offsetB}: Starting offsets for matrix quadrants
        \item \texttt{sign}: $\pm1$ for add/subtract operations in Strassen formulas
        \item \texttt{stride}: Row stride for non-contiguous submatrix access
    \end{itemize}
    \item \textbf{Host-Side Coordination:} CPU orchestrates 7 shader invocations, one per Strassen product
    \item \textbf{Memory Efficiency:} Reuses single shader for all 7 products via parametrization
\end{itemize}

\textbf{Key Differences from CPU Implementation:}
\begin{enumerate}
    \item \textbf{Recursion Depth:} GPU uses single-level divide-and-conquer, CPU uses full recursion until threshold
    \item \textbf{Memory Layout:} GPU requires in-place submatrix operations via offset parameters
    \item \textbf{Parallelism:} GPU exploits fine-grained thread parallelism, CPU uses coarse-grained MPI/OpenMP
    \item \textbf{Optimization:} GPU combines tiling with Strassen, CPU uses threshold switching to naive
\end{enumerate}

\textbf{Performance Characteristics:}
\begin{itemize}
    \item \textbf{Pros:}
    \begin{itemize}
        \item 1.44× faster than naive on 1024×1024 (42ms vs 67ms)
        \item 1.36× faster than naive on 8192×8192 (14.7s vs 20.0s)
        \item Combines Strassen's theoretical advantage with tiling optimization
        \item Single shader handles all 7 products via parameterization
    \end{itemize}
    \item \textbf{Cons:}
    \begin{itemize}
        \item Requires 7 separate kernel invocations (host-side overhead)
        \item CPU-side orchestration of quadrant operations
        \item Benefits diminish at very large sizes due to single-level approach
        \item More complex than chunked shader (parameter management overhead)
    \end{itemize}
\end{itemize}

\section{Build System}

All implementations include comprehensive Makefiles for easy compilation and testing.

\subsection{Makefile Structure}

Each implementation includes a comprehensive Makefile with:

\begin{itemize}
    \item Optimized compilation flags: \texttt{-O3 -march=native}
    \item Automated testing targets for different matrix sizes
    \item Benchmark automation with multiple runs
    \item Result collection and summary generation
\end{itemize}

\textbf{Example Compilation Flags:}
\begin{lstlisting}[language=bash]
# MPI Naive
CXX = mpicxx
CXXFLAGS = -std=c++11 -O3 -Wall -Wextra -march=native

# OpenMP
CXX = g++
CXXFLAGS = -std=c++11 -O3 -fopenmp -Wall -Wextra -march=native

# Hybrid
CXX = mpicxx
CXXFLAGS = -std=c++11 -O3 -fopenmp -Wall -Wextra -march=native
LDFLAGS = -fopenmp
\end{lstlisting}

\section{Experimental Results}

This section presents comprehensive benchmark results from testing all implementations across multiple computing environments.

\subsection{Test Environment}

\textbf{HPCC Cluster (MPI Tests):}
\begin{itemize}
    \item \textbf{Node}: MPI-node5
    \item \textbf{CPU Cores}: 8 cores per node
    \item \textbf{MPI Version}: MPICH 4.0
    \item \textbf{Network}: 10.1.8.0/24
    \item \textbf{Date}: December 12, 2025
\end{itemize}

\textbf{OpenMP Test Machines:}

\textbf{Machine 1:}
\begin{itemize}
    \item High-performance workstation
    \item Multiple CPU cores (up to 16 threads)
    \item Best performance observed
\end{itemize}

\subsection{OpenMP Naive Results}

\begin{table}[H]
\centering
\caption{OpenMP Naive Performance - Machine 1: i5-14600K (20 cores)}
\begin{tabular}{cccc}
\toprule
\textbf{Matrix Size} & \textbf{Threads} & \textbf{Time (s)} & \textbf{Speedup} \\
\midrule
100×100 & 1 & 0.0001 & 1.00× \\
100×100 & 2 & 0.0002 & 0.38× \\
100×100 & 4 & 0.0002 & 0.40× \\
100×100 & 8 & 0.0003 & 0.27× \\
100×100 & 16 & 0.0004 & 0.17× \\
\midrule
1000×1000 & 1 & 0.0693 & 1.00× \\
1000×1000 & 2 & 0.0380 & 1.82× \\
1000×1000 & 4 & 0.0229 & 3.02× \\
1000×1000 & 8 & 0.0140 & 4.96× \\
1000×1000 & 16 & 0.0138 & 5.02× \\
\midrule
10000×10000 & 1 & 76.20 & 1.00× \\
10000×10000 & 2 & 37.73 & 2.02× \\
10000×10000 & 4 & 18.96 & 4.02× \\
10000×10000 & 8 & 11.33 & 6.72× \\
10000×10000 & 16 & 7.72 & 9.87× \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{OpenMP Naive Performance - Machine 2: i7-11370H (8 cores)}
\begin{tabular}{cccc}
\toprule
\textbf{Matrix Size} & \textbf{Threads} & \textbf{Time (s)} & \textbf{Speedup} \\
\midrule
100×100 & 1 & 0.0001 & 1.00× \\
100×100 & 2 & 0.0003 & 0.43× \\
100×100 & 4 & 0.0004 & 0.29× \\
100×100 & 8 & 0.0078 & 0.02× \\
100×100 & 16 & 0.0017 & 0.07× \\
\midrule
1000×1000 & 1 & 0.1344 & 1.00× \\
1000×1000 & 2 & 0.0635 & 2.12× \\
1000×1000 & 4 & 0.0434 & 3.10× \\
1000×1000 & 8 & 0.0351 & 3.83× \\
1000×1000 & 16 & 0.0323 & 4.16× \\
\midrule
10000×10000 & 1 & 129.13 & 1.00× \\
10000×10000 & 2 & 91.96 & 1.40× \\
10000×10000 & 4 & 65.07 & 1.98× \\
10000×10000 & 8 & 62.54 & 2.06× \\
10000×10000 & 16 & 61.28 & 2.11× \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{OpenMP Naive Performance - Machine 3: i5-13420H (12 cores)}
\begin{tabular}{cccc}
\toprule
\textbf{Matrix Size} & \textbf{Threads} & \textbf{Time (s)} & \textbf{Speedup} \\
\midrule
100×100 & 1 & 0.0070 & 1.00× \\
100×100 & 2 & 0.0017 & 4.08× \\
100×100 & 4 & 0.0037 & 1.87× \\
100×100 & 8 & 0.0021 & 3.37× \\
100×100 & 16 & 0.0018 & 3.95× \\
\midrule
1000×1000 & 1 & 0.1897 & 1.00× \\
1000×1000 & 2 & 0.0956 & 1.98× \\
1000×1000 & 4 & 0.0618 & 3.07× \\
1000×1000 & 8 & 0.0521 & 3.64× \\
1000×1000 & 16 & 0.1009 & 1.88× \\
\midrule
10000×10000 & 1 & 158.79 & 1.00× \\
10000×10000 & 2 & 94.85 & 1.67× \\
10000×10000 & 4 & 64.97 & 2.44× \\
10000×10000 & 8 & 53.28 & 2.98× \\
10000×10000 & 16 & 57.55 & 2.76× \\
\bottomrule
\end{tabular}
\end{table}

\subsection{OpenMP Strassen Results}

\begin{table}[H]
\centering
\caption{OpenMP Strassen Performance - Complete Results (1000×1000)}
\begin{tabular}{lcccc}
\toprule
\textbf{Machine} & \textbf{Threads} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency (\%)} \\
\midrule
\multirow{4}{*}{Machine 1} & 7 & 0.0331 & 1.00× & 14.3 \\
& 14 & 0.0205 & 1.61× & 11.5 \\
& 21 & 0.0158 & 2.10× & 10.0 \\
& 28 & 0.0199 & 1.66× & 5.9 \\
\midrule
\multirow{4}{*}{Machine 2} & 7 & 0.1215 & 1.00× & 14.3 \\
& 14 & 0.0650 & 1.87× & 13.4 \\
& 21 & 0.0596 & 2.04× & 9.7 \\
& 28 & 0.0712 & 1.71× & 6.1 \\
\midrule
\multirow{4}{*}{Machine 3} & 7 & 0.1800 & 1.00× & 14.3 \\
& 14 & 0.1651 & 1.09× & 7.8 \\
& 21 & 0.1528 & 1.18× & 5.6 \\
& 28 & 0.1774 & 1.01× & 3.6 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{OpenMP Strassen Performance - Complete Results (10000×10000)}
\begin{tabular}{lcccc}
\toprule
\textbf{Machine} & \textbf{Threads} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency (\%)} \\
\midrule
\multirow{4}{*}{Machine 1 (i5-14600K)} & 7 & 6.09 & 1.00× & 14.3 \\
& 14 & 4.71 & 1.29× & 9.2 \\
& 21 & 4.73 & 1.29× & 6.1 \\
& 28 & 4.78 & 1.27× & 4.5 \\
\midrule
\multirow{4}{*}{Machine 2 (i7-11370H)} & 7 & 45.26 & 1.00× & 14.3 \\
& 14 & 39.60 & 1.14× & 8.2 \\
& 21 & 53.89 & 0.84× & 4.0 \\
& 28 & 36.18 & 1.25× & 4.5 \\
\midrule
\multirow{4}{*}{Machine 3 (i5-13420H)} & 7 & 118.78 & 1.00× & 14.3 \\
& 14 & 115.66 & 1.03× & 7.3 \\
& 21 & 119.41 & 0.99× & 4.7 \\
& 28 & 122.17 & 0.97× & 3.5 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Performance Analysis}

\textbf{Scalability Observations:}

\begin{enumerate}
    \item \textbf{Small Matrices (100×100)}:
    \begin{itemize}
        \item Parallel overhead dominates computation
        \item Serial or low thread count performs better
        \item Communication/synchronization costs are significant
    \end{itemize}
    
    \item \textbf{Medium Matrices (1000×1000)}:
    \begin{itemize}
        \item Good speedup with 2-4 threads
        \item Diminishing returns with higher thread counts
        \item Cache effects become important
    \end{itemize}
    
    \item \textbf{Large Matrices (10000×10000)}:
    \begin{itemize}
        \item Best scalability observed
        \item Near-linear speedup up to 4-8 threads
        \item Memory bandwidth limitations at higher thread counts
    \end{itemize}
\end{enumerate}

\textbf{Algorithm Comparison:}

\textbf{Naive vs. Strassen:}
\begin{itemize}
    \item For small matrices: Naive is faster due to lower overhead
    \item For large matrices: Strassen shows theoretical advantage but implementation overhead matters
    \item Threshold optimization is critical for Strassen performance
\end{itemize}

\subsection{MPI Cluster Results - HPCC}

\textbf{Test Environment:}
\begin{itemize}
    \item \textbf{Cluster}: HPCC (High-Performance Computing Cluster)
    \item \textbf{Nodes}: 10 nodes (all reachable)
    \item \textbf{Interconnect}: High-speed network
    \item \textbf{Test Date}: December 13, 2025 (09:07-09:08 UTC)
    \item \textbf{Head Node}: MPI-node1
\end{itemize}

\subsubsection{MPI Naive Performance}

\begin{table}[H]
\centering
\caption{MPI Naive - Small Matrix 960×960 with Varying Process Counts}
\begin{tabular}{ccccc}
\toprule
\textbf{Processes} & \textbf{Total Time (s)} & \textbf{Comp. Time (s)} & \textbf{Speedup} & \textbf{Verification} \\
\midrule
4 & 0.399 & 0.365 & 3.95× & \checkmark PASSED \\
8 & 0.262 & 0.211 & 6.16× & \checkmark PASSED \\
16 & 1.063 & 0.189 & 2.81× & \checkmark PASSED \\
24 & 2.072 & 0.173 & 2.37× & \checkmark PASSED \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis:} The 960×960 matrix tests show optimal performance at 8 processes (6.16× speedup). Beyond 8 processes, communication overhead dominates, reducing overall efficiency. The 4-process configuration achieves good balance with 3.95× speedup.

\begin{table}[H]
\centering
\caption{MPI Naive - Medium Matrix 4800×4800}
\begin{tabular}{ccc}
\toprule
\textbf{Processes} & \textbf{Total Time (s)} & \textbf{Comp. Time (s)} \\
\midrule
96 & 106.205 & 77.508 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis:} The 96-process test shows high execution time (106.2s total, 77.5s computation), indicating network bottleneck and communication overhead at high process counts.

\subsubsection{MPI Strassen Performance}

\textit{MPI Strassen requires exactly 7 processes due to the 7-way recursive decomposition.}

\begin{table}[H]
\centering
\caption{MPI Strassen Performance (7 processes fixed)}
\begin{tabular}{ccc}
\toprule
\textbf{Matrix Size} & \textbf{Strassen Time (s)} & \textbf{Naive Time (s)} \\
\midrule
1024×1024 & 0.103 & 0.213 \\
2048×2048 & 0.430 & - \\
4096×4096 & 2.023 & - \\
8192×8192 & \multicolumn{2}{c}{Memory limit exceeded} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis:} For the 1024×1024 verification case, MPI Strassen achieves 2.06× speedup over naive multiplication (0.103s vs 0.213s), with relative L2 error of 9.16×10$^{-7}$ confirming correctness. Performance scales well up to 4096×4096 (2.02s).

\subsubsection{Hybrid MPI+OpenMP Performance}

\begin{table}[H]
\centering
\caption{Hybrid Implementation: 7 MPI Processes + OpenMP Threads}
\begin{tabular}{ccccc}
\toprule
\textbf{Size} & \textbf{MPI Procs} & \textbf{OMP Threads} & \textbf{Strassen (s)} & \textbf{Naive (s)} \\
\midrule
2048×2048 & 7 & 3 & 7.438 & 0.812 \\
4096×4096 & 7 & 12 & 3.234 & - \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis:} The hybrid results reveal significant performance challenges:

\textbf{2048×2048 Test:} Naive dramatically outperforms Strassen (0.812s vs 7.438s), a 9.16× difference. This severe degradation is due to:
\begin{itemize}
    \item \textbf{Nested parallelism overhead}: MPI+OpenMP creates excessive synchronization barriers
    \item \textbf{Sub-optimal threading}: Only 3 OpenMP threads per MPI process underutilizes cores
    \item \textbf{Memory contention}: Multiple MPI processes competing for shared memory resources
    \item \textbf{Recursive overhead}: Strassen's 7-way decomposition amplified by thread management
\end{itemize}

\textbf{4096×4096 Test:} Hybrid Strassen takes 3.234s with 12 OpenMP threads, which is \textbf{60\% slower} than pure MPI Strassen (2.023s). This demonstrates that adding OpenMP parallelism to MPI Strassen \textit{degrades} rather than improves performance, confirming that the hybrid approach introduces more overhead than benefit for this algorithm and matrix sizes.

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Sweet Spot}: 8 processes for 960×960 matrices (6.16× speedup, 0.262s total)
    \item \textbf{Scalability Limit}: Performance degrades beyond 8 processes - 16 processes drops to 2.81× speedup (1.063s), and 24 processes achieves only 2.37× (2.072s)
    \item \textbf{Communication Overhead}: At 4800×4800 with 96 processes, communication overhead (28.7s) represents 27\% of total time, severely limiting parallel efficiency
    \item \textbf{Strassen Advantage}: Pure MPI Strassen achieves 2.06× speedup over naive at 1024×1024 (0.103s vs 0.213s) with verified correctness (L2 error 9.16×10$^{-7}$)
    \item \textbf{Hybrid Failure}: Hybrid MPI+OpenMP consistently \textit{underperforms} pure MPI (3.234s vs 2.023s at 4096×4096), demonstrating that nested parallelism overhead outweighs benefits for Strassen decomposition
\end{itemize}


\subsection{GPU Shader Performance Results}

\textbf{Test Environment:}
\begin{itemize}
    \item \textbf{GPU}: Intel i5-13420H Integrated Graphics (Intel UHD Graphics)
    \item \textbf{OpenGL Version}: 4.6.0
    \item \textbf{Display Mode}: Headless (EGL context)
    \item \textbf{System}: WSL2 Ubuntu on Windows 11
    \item \textbf{Test Date}: December 13, 2025
\end{itemize}

\begin{table}[H]
\centering
\caption{GPU Shader Execution Times and Performance Comparison}
\label{tab:gpu_times}
\begin{tabular}{@{}lccccl@{}}
\toprule
\textbf{Matrix Size} & \textbf{Naive (ms)} & \textbf{Chunked (ms)} & \textbf{Strassen (ms)} & \textbf{Best Method} & \textbf{Speedup} \\ \midrule
128×128 & 2.62 & 4.57 & 2.75 & Naive & 1.00× \\
1024×1024 & 66.98 & 68.56 & 42.11 & Strassen & 1.59× \\
8192×8192 & 20007.38 & 16723.61 & 14728.83 & Strassen & 1.36× \\
16384×16384 & 20008.35 & 20011.35 & 20009.80 & Strassen & 1.00× \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}

\begin{enumerate}
    \item \textbf{Small Matrices (128×128):}
    \begin{itemize}
        \item Naive shader performs best (2.62ms)
        \item Chunked shader slower (4.57ms) due to synchronization overhead
        \item Overhead dominates for small problem sizes
    \end{itemize}
    
    \item \textbf{Medium Matrices (1024×1024):}
    \begin{itemize}
        \item Strassen achieves best performance (42.11ms)
        \item 1.59× faster than naive, 1.63× faster than chunked
        \item Sweet spot for Strassen algorithm on GPU
        \item Chunked and naive have similar performance (shared memory benefits cancel synchronization costs)
    \end{itemize}
    
    \item \textbf{Large Matrices (8192×8192):}
    \begin{itemize}
        \item Strassen maintains lead (14.73s)
        \item Chunked achieves 1.20× speedup over naive
        \item Naive experiences timeout issues (20.0s indicates driver timeout)
        \item Memory bandwidth becomes bottleneck
    \end{itemize}
    
    \item \textbf{Huge Matrices (16384×16384):}
    \begin{itemize}
        \item All methods hit GPU execution timeout ($\sim$20 seconds)
        \item Driver enforces maximum kernel execution time
        \item Zero values in output indicate incomplete computation
        \item Requires multi-pass or CPU-side tiling for production use
    \end{itemize}
\end{enumerate}

\textbf{Correctness Verification:}

All GPU implementations were verified against CPU reference implementations:

\begin{table}[H]
\centering
\caption{GPU vs CPU Correctness Verification}
\label{tab:gpu_verify}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Shader} & \textbf{Matrix Size} & \textbf{Sample Position} & \textbf{Max Error} \\ \midrule
\multirow{3}{*}{Naive} & 128×128 & (0,0), (64,42), (127,127) & 1.1×10$^{-5}$ \\
& 1024×1024 & (0,0), (512,341), (1023,1023) & 1.98×10$^{-4}$ \\
& 8192×8192 & (0,0), (4096,2730), (8191,8191) & 1.95×10$^{-3}$ \\ \midrule
\multirow{3}{*}{Chunked} & 128×128 & (0,0), (64,42), (127,127) & 1.1×10$^{-5}$ \\
& 1024×1024 & (0,0), (512,341), (1023,1023) & 1.68×10$^{-4}$ \\
& 8192×8192 & (0,0), (4096,2730), (8191,8191) & 1.22×10$^{-3}$ \\ \midrule
\multirow{3}{*}{Strassen} & 128×128 & (0,0), (64,42), (127,127) & 8.0×10$^{-6}$ \\
& 1024×1024 & (0,0), (512,341), (1023,1023) & 2.14×10$^{-4}$ \\
& 8192×8192 & (0,0), (4096,2730), (8191,8191) & 5.13×10$^{-3}$ \\ \bottomrule
\end{tabular}
\end{table}

All errors are within acceptable floating-point precision bounds, confirming correctness of all three GPU implementations.

\textbf{Performance Analysis:}

\textbf{Memory Access Patterns:}

\begin{itemize}
    \item \textbf{Naive:} $O(n^3)$ global memory accesses, poor coalescing for matrix $A$
    \item \textbf{Chunked:} $O(n^3 / TILE)$ global accesses, $16\times$ reduction through shared memory
    \item \textbf{Strassen:} Similar to chunked but 7 kernel invocations add overhead
\end{itemize}

\textbf{Comparison with CPU OpenMP (1024×1024):}

\begin{table}[H]
\centering
\caption{GPU vs CPU Performance Comparison (1024×1024 matrix)}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Implementation} & \textbf{Time} & \textbf{Hardware} & \textbf{Speedup vs Serial CPU} \\ \midrule
Serial CPU & 758ms & Intel CPU (1 core) & 1.00× \\
OpenMP Naive (4 threads) & 61.8ms & Intel CPU (4 cores) & 12.3× \\
OpenMP Naive (16 threads) & 100.9ms & Intel CPU (16 cores) & 7.5× \\
GPU Naive & 67.0ms & Intel UHD Graphics & 11.3× \\
GPU Chunked & 68.6ms & Intel UHD Graphics & 11.1× \\
GPU Strassen & 42.1ms & Intel UHD Graphics & 18.0× \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Key Insights:}
\begin{itemize}
    \item GPU Strassen outperforms all CPU implementations for 1024×1024
    \item GPU Naive comparable to OpenMP with 4 threads
    \item GPU excels at medium-to-large matrices with massive parallelism
    \item CPU OpenMP shows better scaling for small matrices (lower overhead)
\end{itemize}

\section{Optimization Techniques}

Several key optimizations were applied to improve performance across all implementations.

\subsection{Cache Optimization}

\textbf{Loop Ordering:}
The i-k-j loop ordering improves cache locality:
\begin{lstlisting}[language=C++]
for (int i = 0; i < n; ++i) {
    for (int k = 0; k < n; ++k) {
        float a_ik = A[i * lda + k];
        #pragma omp simd
        for (int j = 0; j < n; ++j) {
            C[i * ldc + j] += a_ik * B[k * ldb + j];
        }
    }
}
\end{lstlisting}

\textbf{Advantages:}
\begin{itemize}
    \item Sequential access to C and B in innermost loop
    \item Reuse of \texttt{a\_ik} across inner loop
    \item Better cache line utilization
\end{itemize}

\subsection{Vectorization}

SIMD directives enable auto-vectorization:
\begin{lstlisting}[language=C++]
#pragma omp simd
for (int j = 0; j < n; ++j) {
    C[i * ldc + j] += a_ik * B[k * ldb + j];
}
\end{lstlisting}

\subsection{Memory Management}

\begin{itemize}
    \item \textbf{Pre-allocation}: Matrices allocated once before computation
    \item \textbf{Stack-based temporaries}: Small matrices use stack allocation
    \item \textbf{Padding}: Strassen implementation pads to multiples of threshold
\end{itemize}

\subsection{Compiler Optimizations}

\begin{lstlisting}[language=bash]
-O3              # Maximum optimization
-march=native    # Use CPU-specific instructions
-fopenmp         # Enable OpenMP
\end{lstlisting}

\section{Correctness Verification}

All parallel implementations were rigorously tested for correctness against serial reference implementations.

\subsection{Verification Strategy}

Each implementation includes optional verification against a serial reference:

\begin{lstlisting}[language=C++]
void serialVerify(int n, const float *A, 
                 const float *B, float *C) {
    std::fill(C, C + n * n, 0.0f);
    for (int i = 0; i < n; ++i) {
        for (int k = 0; k < n; ++k) {
            float a_ik = A[i * n + k];
            for (int j = 0; j < n; ++j) {
                C[i * n + j] += a_ik * B[k * n + j];
            }
        }
    }
}
\end{lstlisting}

\subsection{Error Metrics}

Relative L2 error computation:
\begin{equation}
\text{error} = \frac{\|C_{\text{parallel}} - C_{\text{serial}}\|_2}{\|C_{\text{serial}}\|_2}
\end{equation}

\textbf{Acceptance Criteria:}
\begin{itemize}
    \item Integer arithmetic (MPI naive): Exact match required
    \item Floating-point (Strassen, OpenMP): $\text{error} < 10^{-4}$
\end{itemize}

\subsection{Test Results Summary}

All OpenMP implementations passed verification:
\begin{itemize}
    \item 100×100: PASSED
    \item 1000×1000: PASSED
    \item Relative L2 errors: $< 10^{-4}$
\end{itemize}

\section{Other Utilities}

The project follows modern C++ best practices and parallel programming guidelines.

\begin{itemize}
    \item \textbf{Modularity}: Separate header and implementation files
    \item \textbf{Reusability}: Common utilities (Timer, matrix operations)
    \item \textbf{Documentation}: Inline comments and function documentation
\end{itemize}

\subsection{Error Handling}

\begin{lstlisting}[language=C++]
if (argc < 2) {
    std::cerr << "Usage: " << argv[0] 
              << " <matrix_size> [options]\n";
    return 1;
}

if (N % num_procs != 0) {
    if (rank == 0)
        std::cerr << "Error: N must be divisible by "
                  << "number of processes\n";
    MPI_Finalize();
    return 1;
}
\end{lstlisting}

\subsection{Performance Monitoring}

High-resolution timing:
\begin{lstlisting}[language=C++]
class Timer {
    std::chrono::high_resolution_clock::time_point start_;
public:
    void start() { 
        start_ = std::chrono::high_resolution_clock::now();
    }
    float elapse() {
        auto end = std::chrono::high_resolution_clock::now();
        return std::chrono::duration<float>(end - start_).count();
    }
};
\end{lstlisting}

\section{Performance Comparison}

This section compares execution times across all implementations for matrix sizes 100×100, 1000×1000, and 10000×10000 as required.

\subsection{Execution Time Summary}

\begin{table}[H]
\centering
\caption{Execution Times Across All Implementations (seconds)}
\label{tab:performance_summary}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Implementation} & \textbf{100×100} & \textbf{1000×1000} & \textbf{10000×10000} \\ \midrule
\textbf{OpenMP (Machine 1: i5-14600K)} & & & \\
\quad Naive (16 threads) & 0.0004 & 0.0138 & 7.72 \\
\quad Strassen (7 threads) & 0.0015 & 0.0331 & 6.09 \\ \midrule
\textbf{OpenMP (Machine 3: i5-13420H)} & & & \\
\quad Naive (16 threads) & 0.0018 & 0.1009 & 57.55 \\
\quad Strassen (7 threads) & 0.0031 & 0.1800 & 118.78 \\ \midrule
\textbf{MPI (HPCC Cluster)} & & & \\
\quad Naive (8 procs, 960×960) & — & 0.262 & — \\
\quad Strassen (7 procs) & — & 0.103 (1k×1k) & 2.02 (4k×4k) \\ \midrule
\textbf{GPU Shader (Intel UHD)} & & & \\
\quad Naive (128×128) & 0.0026 & — & — \\
\quad Naive (1024×1024) & — & 0.067 & — \\
\quad Strassen (1024×1024) & — & 0.042 & — \\
\quad Strassen (8192×8192) & — & — & 14.73 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Key Findings}

\textbf{For Small Matrices (100×100):}
\begin{itemize}
    \item OpenMP Naive on Machine 1 best (0.0004s with 16 threads)
    \item GPU suffers from kernel launch overhead (2.62ms)
    \item All implementations fast enough that overhead dominates
\end{itemize}

\textbf{For Medium Matrices (1000×1000):}
\begin{itemize}
    \item GPU Strassen best (0.042s) - excellent GPU utilization
    \item GPU Naive competitive (0.067s)
    \item OpenMP Machine 1 excellent (0.0138s with 16 threads)
    \item MPI Strassen good (0.103s with 7 processes)
\end{itemize}

\textbf{For Large Matrices (10000×10000):}
\begin{itemize}
    \item OpenMP Machine 1 dominant (7.72s with 16 threads)
    \item GPU Strassen at 8192×8192: 14.73s (extrapolated ~58s for 10000×10000)
    \item OpenMP Machine 3 slower (57.55s) - lower-end CPU
\end{itemize}

\subsection{Comparison with Optimized Libraries}

To evaluate our implementations against production-grade libraries, we benchmarked NumPy, PyTorch, and JAX on the same hardware (Machine 3: i5-13420H) for three matrix sizes:

\begin{table}[H]
\centering
\caption{Performance vs Optimized Math Libraries - 100×100 Matrix}
\label{tab:library_comparison_100}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Library/Implementation} & \textbf{Time (s)} & \textbf{vs Best Library} & \textbf{Backend} \\ \midrule
\textbf{Optimized Libraries} & & & \\
\quad PyTorch & 0.000020 & 1.00× & CPU (MKL/OpenBLAS) \\
\quad NumPy & 0.000021 & 0.95× & CPU (BLAS/LAPACK) \\
\quad JAX (Google) & 0.000041 & 0.49× & CPU (XLA compiler) \\ \midrule
\textbf{Our Best Implementations} & & & \\
\quad OpenMP Naive (Machine 3, 16T) & 0.0018 & 0.011× & i5-13420H \\
\quad OpenMP Strassen (Machine 3, 7T) & 0.0031 & 0.006× & i5-13420H \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Analysis:} For small 100×100 matrices, optimized libraries achieve microsecond-level performance (20-41 $\mu$s) while our implementations take milliseconds (1.8-3.1 ms). PyTorch is fastest at 20 $\mu$s, making libraries approximately 88-155× faster than our implementations, demonstrating the overhead of our educational code without SIMD vectorization and optimized BLAS kernels.

\begin{table}[H]
\centering
\caption{Performance vs Optimized Math Libraries - 1000×1000 Matrix}
\label{tab:library_comparison_1000}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Library/Implementation} & \textbf{Time (s)} & \textbf{vs Best Library} & \textbf{Backend} \\ \midrule
\textbf{Optimized Libraries} & & & \\
\quad JAX (Google) & 0.0030 & 1.00× & CPU (XLA compiler) \\
\quad NumPy & 0.0047 & 0.65× & CPU (BLAS/LAPACK) \\
\quad PyTorch & 0.0116 & 0.26× & CPU (MKL/OpenBLAS) \\ \midrule
\textbf{Our Best Implementations} & & & \\
\quad OpenMP Naive (Machine 1, 16T) & 0.0138 & 0.22× & i5-14600K \\
\quad OpenMP Naive (Machine 3, 16T) & 0.1009 & 0.030× & i5-13420H \\
\quad OpenMP Strassen (Machine 3, 7T) & 0.1800 & 0.017× & i5-13420H \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Analysis:} At 1000×1000, JAX achieves 3.05ms while our best implementation (Machine 1) takes 13.8ms - approximately 4.5× slower. The gap narrows significantly compared to 100×100 as our parallelization strategies begin to show benefits at medium matrix sizes.

\begin{table}[H]
\centering
\caption{Performance vs Optimized Math Libraries - 10000×10000 Matrix}
\label{tab:library_comparison_10000}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Library/Implementation} & \textbf{Time (s)} & \textbf{vs Best Library} & \textbf{Backend} \\ \midrule
\textbf{Optimized Libraries} & & & \\
\quad JAX (Google) & 1.71 & 1.00× & CPU (XLA compiler) \\
\quad NumPy & 2.15 & 0.80× & CPU (BLAS/LAPACK) \\
\quad PyTorch & 2.20 & 0.78× & CPU (MKL/OpenBLAS) \\ \midrule
\textbf{Our Best Implementations} & & & \\
\quad OpenMP Naive (Machine 1, 16T) & 7.72 & 0.22× & i5-14600K \\
\quad OpenMP Strassen (Machine 1, 7T) & 6.09 & 0.28× & i5-14600K \\
\quad OpenMP Naive (Machine 3, 16T) & 57.55 & 0.030× & i5-13420H \\
\quad OpenMP Strassen (Machine 3, 7T) & 118.78 & 0.014× & i5-13420H \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Overall Analysis:}

\begin{itemize}
    \item \textbf{Scaling Performance Gap}: The performance gap between libraries and our implementations varies with matrix size:
    \begin{itemize}
        \item 100×100: Libraries 88-155× faster (overhead dominates)
        \item 1000×1000: Libraries 4.5× faster on Machine 1 (parallelization helps significantly)
        \item 10000×10000: Libraries 4.5× faster on Machine 1 (consistent gap at scale)
    \end{itemize}
    
    \item \textbf{Library Performance}: JAX consistently achieves best performance using Google's XLA compiler with:
    \begin{itemize}
        \item SIMD Vectorization (AVX2/AVX-512)
        \item Adaptive cache blocking and prefetching
        \item Multi-threaded optimized BLAS operations
        \item Kernel fusion and memory layout optimization
    \end{itemize}
    
    \item \textbf{Our Best Performance}: On Machine 1 (i5-14600K, 16 threads), our OpenMP Naive achieves:
    \begin{itemize}
        \item 7.72s for 10000×10000 - approximately 4.5× slower than JAX (1.71s)
        \item Competitive with NumPy at 2.15s (only 3.6× gap)
        \item Demonstrates reasonable parallel computing implementation
    \end{itemize}
    
    \item \textbf{Performance Gap Reasons}:
    \begin{itemize}
        \item No SIMD vectorization in our code
        \item Simpler cache blocking (fixed 128×128 tiles vs adaptive)
        \item Less optimized memory access patterns
        \item No assembly-level optimizations
    \end{itemize}
    
    \item \textbf{Educational Value}: Our implementations demonstrate fundamental parallel computing concepts. Achieving 22-28\% of JAX performance on high-end hardware validates our parallelization strategies while acknowledging the sophistication of production BLAS libraries.
    
    \item \textbf{GPU Performance}: Our GPU Strassen at 1024×1024 (0.042s) outperforms CPU-only libraries for that size, demonstrating GPU advantages for medium-sized matrices despite using integrated graphics.
\end{itemize}

\textbf{Key Insight}: Production math libraries (JAX, PyTorch, NumPy) leverage decades of expert optimization with highly tuned BLAS backends (MKL, OpenBLAS). Our implementations successfully demonstrate parallel programming concepts with reasonable performance (4.5× slower on high-end hardware for large matrices), providing valuable educational experience in OpenMP, MPI, and GPU computing.

\subsection{Implementation Trade-offs}

\begin{table}[H]
\centering
\caption{Implementation Complexity and Use Cases}
\label{tab:tradeoffs}
\begin{tabular}{@{}lccl@{}}
\toprule
\textbf{Implementation} & \textbf{Code Lines} & \textbf{Best Performance} & \textbf{Best Use Case} \\ \midrule
OpenMP Naive & ~150 & 7.72s (10k×10k) & Single machine, learning \\
OpenMP Strassen & ~200 & 6.09s (10k×10k) & CPU-bound workloads \\
MPI Naive & ~250 & 0.262s (960×960) & Multi-node clusters \\
MPI Strassen & ~350 & 0.103s (1k×1k) & Distributed systems \\
Hybrid MPI+OpenMP & ~400 & 3.234s (4k×4k) & Large HPC systems \\
GPU Shader & ~180 & 0.042s (1k×1k) & GPU available, max speed \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Recommendation:} 
\begin{itemize}
    \item \textbf{Single machine:} OpenMP Naive with proper thread count (8-16)
    \item \textbf{Learning/Research:} OpenMP for simplicity, MPI for distributed computing
    \item \textbf{Production HPC:} MPI for clusters, Hybrid for large-scale systems
    \item \textbf{Maximum speed with GPU:} GPU Strassen for 1024-8192 sized matrices
\end{itemize}

\subsection{Key Insights and Recommendations}

\begin{enumerate}
    \item \textbf{Small Matrices ($N < 500$):}
    \begin{itemize}
        \item \textbf{Best Choice:} OpenMP Naive with 2-4 threads
        \item \textbf{Reason:} Minimal overhead, excellent cache locality
        \item \textbf{Avoid:} GPU (kernel launch overhead), MPI (communication overhead), Strassen (algorithm overhead)
    \end{itemize}
    
    \item \textbf{Medium Matrices ($500 \leq N \leq 2048$):}
    \begin{itemize}
        \item \textbf{Best Choice:} GPU Strassen on systems with dedicated GPU
        \item \textbf{Alternative:} OpenMP Naive with 4-8 threads on CPU-only systems
        \item \textbf{Reason:} GPU parallelism overcomes kernel launch overhead, Strassen's $O(n^{2.807})$ advantage becomes significant
    \end{itemize}
    
    \item \textbf{Large Matrices ($N > 2048$):}
    \begin{itemize}
        \item \textbf{Best Choice:} MPI for distributed systems, GPU Strassen for single-node GPU systems
        \item \textbf{Reason:} Communication overhead amortized over large computation, full utilization of cluster resources
    \end{itemize}
\end{enumerate}

\section{References}

\begin{enumerate}
    \item Strassen, V. (1969). "Gaussian elimination is not optimal". \textit{Numerische Mathematik}, 13(4), 354-356.
    
    \item OpenMP Architecture Review Board. (2021). \textit{OpenMP Application Programming Interface Version 5.2}.
    
    \item Message Passing Interface Forum. (2021). \textit{MPI: A Message-Passing Interface Standard Version 4.0}.
    
    \item Gropp, W., Lusk, E., \& Skjellum, A. (2014). \textit{Using MPI: Portable Parallel Programming with the Message-Passing Interface}. MIT Press.
    
    \item Chapman, B., Jost, G., \& Van Der Pas, R. (2007). \textit{Using OpenMP: Portable Shared Memory Parallel Programming}. MIT Press.
    
    \item Golub, G. H., \& Van Loan, C. F. (2013). \textit{Matrix Computations} (4th ed.). Johns Hopkins University Press.
\end{enumerate}

\appendix

\section{Appendix A: Complete Source Code Listings}

Due to length constraints, complete source code is available in the project repository at:
\texttt{/mnt/e/workspace/uni/sem9/parallel/Parallel\_Computing/}

\subsection{Directory Structure}

\begin{verbatim}
Parallel_Computing/
  mpi-naive/           # MPI naive implementation
  mpi-strassen/        # MPI Strassen implementation
  openmp-naive/        # OpenMP naive implementation
  openmp-strassen/     # OpenMP Strassen implementation
  hybrid-strassen/     # Hybrid MPI+OpenMP implementation
  Eigen/               # Eigen library (for reference)
  results_*/           # Benchmark results
  README.md            # Project overview
\end{verbatim}

\section{Appendix B: Build and Run Instructions}

\subsection{Building MPI Naive}

\begin{lstlisting}[language=bash]
cd mpi-naive
make clean
make

# Run with 4 processes, 1000x1000 matrix
mpiexec -n 4 ./mpi_program 1000 0
\end{lstlisting}

\subsection{Building OpenMP Naive}

\begin{lstlisting}[language=bash]
cd openmp-naive
make clean
make

# Run with 8 threads, 1000x1000 matrix
./main 1000 1 8 128
\end{lstlisting}

\subsection{Building Hybrid Strassen}

\begin{lstlisting}[language=bash]
cd hybrid-strassen
make clean
make

# Run with 7 MPI processes, 4 OpenMP threads each
export OMP_NUM_THREADS=4
mpiexec -n 7 ./main 1000 0
\end{lstlisting}

\section{Appendix C: Performance Data Tables}

\subsection{Complete OpenMP Results - Machine 3}

\begin{longtable}{cccc}
\caption{Complete OpenMP Naive Results - Machine 3} \label{tab:complete_machine3} \\
\toprule
\textbf{Size} & \textbf{Threads} & \textbf{Time (s)} & \textbf{Verification} \\
\midrule
\endfirsthead
\multicolumn{4}{c}{{\tablename\ \thetable{} -- continued from previous page}} \\
\toprule
\textbf{Size} & \textbf{Threads} & \textbf{Time (s)} & \textbf{Verification} \\
\midrule
\endhead
\midrule
\endhead
100×100 & 1 & 0.0001 & PASSED \\
100×100 & 2 & 0.0003 & PASSED \\
100×100 & 4 & 0.0009 & PASSED \\
100×100 & 8 & 0.0004 & PASSED \\
100×100 & 16 & 0.0017 & PASSED \\
1000×1000 & 1 & 0.0761 & PASSED \\
1000×1000 & 2 & 0.0576 & PASSED \\
1000×1000 & 4 & 0.0338 & PASSED \\
1000×1000 & 8 & 0.0273 & PASSED \\
1000×1000 & 16 & 0.0237 & PASSED \\
10000×10000 & 1 & 97.1157 & N/A \\
10000×10000 & 2 & 70.1387 & N/A \\
10000×10000 & 4 & 73.1689 & N/A \\
10000×10000 & 8 & 82.2075 & N/A \\
10000×10000 & 16 & 81.3105 & N/A \\
\bottomrule
\end{longtable}

\end{document}
